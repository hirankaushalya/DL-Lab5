{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN85d/2WxNijZJNy8YRHHtK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hirankaushalya/DL-Lab5/blob/main/It21287022_q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6uNMX7UsZ9-T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and Preprocess the Dataset\n",
        "def load_data(file_path):\n",
        "    # Load the dataset (e.g., IMDB movie reviews dataset)\n",
        "    df = pd.read_csv(file_path, engine='python', on_bad_lines='skip')  # Using 'python' engine and skipping bad lines\n",
        "    df.dropna(inplace=True)  # Drop any rows with missing values\n",
        "    return df['review'], df['sentiment']  # Assuming 'review' and 'sentiment' columns"
      ],
      "metadata": {
        "id": "j_MLhTPicfWG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the text\n",
        "def clean_text(text):\n",
        "    # Remove unwanted characters, numbers, and symbols\n",
        "    text = re.sub(r\"[^A-Za-z\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "HrUde1B5cuS6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and Pad Sequences\n",
        "def preprocess_text(reviews, max_words=5000, max_len=200):\n",
        "    reviews = [clean_text(review) for review in reviews]  # Clean the reviews\n",
        "    tokenizer = Tokenizer(num_words=max_words)\n",
        "    tokenizer.fit_on_texts(reviews)\n",
        "    sequences = tokenizer.texts_to_sequences(reviews)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "    return padded_sequences, tokenizer"
      ],
      "metadata": {
        "id": "w_6d963lcwDF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Sentiments\n",
        "def encode_labels(sentiments):\n",
        "    sentiments = sentiments.map({'positive': 1, 'negative': 0}).values\n",
        "    return sentiments"
      ],
      "metadata": {
        "id": "Y3aRUJIVcxvz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "file_path = './IMDB Dataset.csv'  # <-- Provide the correct path to the dataset\n",
        "reviews, sentiments = load_data(file_path)"
      ],
      "metadata": {
        "id": "_KAWqrMCc0_3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess Text Data\n",
        "max_words = 6000  # Consider the top 5000 words\n",
        "max_len = 200  # Pad or truncate reviews to 200 words\n",
        "X, tokenizer = preprocess_text(reviews, max_words=max_words, max_len=max_len)"
      ],
      "metadata": {
        "id": "Q5bZDE5Mc3u5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Sentiments (positive -> 1, negative -> 0)\n",
        "y = encode_labels(sentiments)\n",
        "\n",
        "# Split into Training and Testing Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "oQkw7ml8c6hM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define the LSTM Model\n",
        "model = Sequential()\n",
        "\n",
        "# Modify the embedding dimensions and experiment with LSTM configurations ---\n",
        "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))  # <-- Modify 'output_dim'\n",
        "model.add(Bidirectional(LSTM(units=74, return_sequences=False)))  # <-- Experiment with 'units' and add Dropout if necessary\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 3. Train the Model\n",
        "#  Modify 'epochs' and 'batch_size' to see how they impact training time and model accuracy ---\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=52, validation_data=(X_test, y_test), verbose=1)  # <-- Experiment with 'epochs' and 'batch_size'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAd_erf6c75Q",
        "outputId": "a3d7a9f4-0b93-4c22-885d-dc0ea9d5bf24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 498ms/step - accuracy: 0.7364 - loss: 0.5073 - val_accuracy: 0.8545 - val_loss: 0.3301\n",
            "Epoch 2/5\n",
            "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 494ms/step - accuracy: 0.8908 - loss: 0.2694 - val_accuracy: 0.8768 - val_loss: 0.2956\n",
            "Epoch 3/5\n",
            "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 490ms/step - accuracy: 0.9183 - loss: 0.2133 - val_accuracy: 0.8877 - val_loss: 0.2858\n",
            "Epoch 4/5\n",
            "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 496ms/step - accuracy: 0.9363 - loss: 0.1669 - val_accuracy: 0.8840 - val_loss: 0.3044\n",
            "Epoch 5/5\n",
            "\u001b[1m706/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m29s\u001b[0m 458ms/step - accuracy: 0.9490 - loss: 0.1386"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Evaluate the Model\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS0ZQFiEc_Jz",
        "outputId": "32818f98-c6a5-485d-9c9c-590aeadfa4f9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 71ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Accuracy and F1-Score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'F1-Score: {f1:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A6gIQIzc_wq",
        "outputId": "c90b944f-050f-494a-f565-33e3b664827c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8828\n",
            "F1-Score: 0.8864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r5ivV3RedBRT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}